# üìö  Understanding and Building Data Pipelines with DAO

## The Problem

Imagine you're a data engineer. Your job is simple in theory: move data from sources to storage to a warehouse. In practice, it's messy.

You write code that directly calls S3, specifying bucket names and credentials. You manage Spark sessions and worry about file formats. You craft JDBC URLs for Redshift connections. Every pipeline step has its own way of doing things.

Then one day, your company says: "We're switching from S3 to Delta Lake." Or: "We need to load this data to Snowflake instead." Your heart sinks. You know what's coming: rewrite half your pipelines. The business logic is buried under storage boilerplate.

**The root problem**: Your code is tightly coupled to storage.

## The Solution: DAO (Data Access Object)

DAO flips the script. Instead of your code knowing *how* to talk to storage, you tell DAO *what* data you want. DAO figures out *how*.

### The Three Building Blocks

**1. DataStore** ‚Äî A ***Data Store*** refers to a complete or a segment of a storage layer which usually are Block storages,
File systems, Object storages or Databases ‚Äî A logical home for your data

- An S3 bucket, Database schema, or Delta location
- A Filesystem Directory or a Prefix in Object Storage

**2. DataObject** ‚Äî Unit of data within a DataStore
- A table, file or a object
- Files in a Directory, Tables in a Schema

**3. Interface** ‚Äî the handler that does the I/O
- Backend-specific implementations
- Each interface knows how to access and perform various operations on all DataObjects in its DataStore
- You don't call interfaces directly; DAO routes to them

### Visual: How They Connect


![data_store.png](examples/docs/img.png)

In this diagram:
- The **large rectangles** = DataStore (logical storage layers)
- The **small rectangles inside** = DataObjects (datasets within each store)
- Each DataObject belongs to exactly one DataStore

![py_objects](examples/docs/data_store_py_relations.png)

In the above diagram, we can see how python objects(Virtual) relate to the Data Store and Data Objects (Real):
- A data store is represented by a DataStore class object
- A data object is represented by a DataObject class object which is associated with a DataStore class object
- Different data stores and data objects have different configurations and properties
- An interface class object is associated with a DataStore class object to perform the data access operations on the DataObjects present in the DataStore
- We can add multiple Interface class objects to a DataStore class object to have multiple ways to access the DataObjects present in the DataStore

- The Data Access Object(DAO) class object is the main entry point to perform data access operations on the DataObjects present in various DataStores
 
### One API, Many Backends

```python
# Same code works for S3, Redshift, Delta, or whatever you add next
dao.read(data_object=my_data)
dao.write(data=df, data_object=my_data, format='parquet')
```

Why this matters
- ‚úÖ **Storage-agnostic code**: Change backends without rewriting pipelines
- ‚úÖ **Clean separation**: I/O logic in interfaces, business logic in your code
- ‚úÖ **Easy configuration**: Register stores once, use them everywhere

---

## üèõÔ∏è The Medallion Architecture: A Real-World Example

Now let's apply DAO to a common data engineering pattern: the **medallion architecture**. Think of it as three zones in a data lake, each serving a different purpose and using different storage.

**The Journey of Data**

Raw data lands ‚Üí gets cleaned ‚Üí becomes analytics-ready. Each zone handles one job:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Data's Journey in a Lake                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  Raw Sources    Bronze Layer    Silver Layer    Gold Layer      ‚îÇ
‚îÇ  (CSV, JSON)       (S3)           (S3)          (Redshift)      ‚îÇ
‚îÇ      ‚îÇ               ‚îÇ              ‚îÇ              ‚îÇ             ‚îÇ
‚îÇ  users.csv ‚îÄ‚îÄ>  users_raw  ‚îÄ‚îÄ>  users_clean  ‚îÄ‚îÄ>  users_agg    ‚îÇ
‚îÇ  events.json ‚îÄ> events_raw ‚îÄ‚îÄ>  events_enr  ‚îÄ‚îÄ>  sales_daily   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  üî¥ Raw         üü† Cleaned      üü° Optimized    üü¢ Analytics    ‚îÇ
‚îÇ  No schema      De-duped        Partitioned    Ready for BI    ‚îÇ
‚îÇ  As-is upload   Type-coerced    Indexed         Aggregated      ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Layer 1: Bronze ‚Äî Raw Ingestion

**What lives here**: Source files exactly as received (CSV, JSON, Parquet dumps)

**Your job**: Upload, store, minimal processing

**Storage**: S3 bucket (`bronze_store`)

**Interface**: `S3Boto3Interface` (simple file operations)

**Example DataObjects**:
- `users_raw` ‚Üí CSV files from user export
- `events_raw` ‚Üí JSON logs from API

```
S3: bronze_bucket/
‚îú‚îÄ‚îÄ users_raw/
‚îÇ   ‚îú‚îÄ‚îÄ 2025-01-07/users.csv
‚îÇ   ‚îî‚îÄ‚îÄ 2025-01-08/users.csv
‚îî‚îÄ‚îÄ events_raw/
    ‚îú‚îÄ‚îÄ 2025-01-07/events.json
    ‚îî‚îÄ‚îÄ 2025-01-08/events.json
```

### Layer 2: Silver ‚Äî Cleaned & Structured

**What lives here**: Deduplicated, typed, partitioned Parquet/Delta tables

**Your job**: Transform, validate schema, partition for performance

**Storage**: S3 with Spark (`silver_store`)

**Interface**: `S3SparkInterface` (Spark read/write)

**Example DataObjects**:
- `users_clean` ‚Üí deduplicated, typed user records
- `events_enriched` ‚Üí events with added metadata

```
S3: silver_bucket/datasets/
‚îú‚îÄ‚îÄ users_clean/
‚îÇ   ‚îú‚îÄ‚îÄ year=2025/month=01/day=07/part-*.parquet
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=01/day=08/part-*.parquet
‚îî‚îÄ‚îÄ events_enriched/
    ‚îú‚îÄ‚îÄ year=2025/month=01/day=07/part-*.parquet
    ‚îî‚îÄ‚îÄ year=2025/month=01/day=08/part-*.parquet
```

### Layer 3: Gold ‚Äî Analytics Ready

**What lives here**: Aggregated, denormalized, joined tables ready for dashboards and reports

**Your job**: Aggregate, join, create facts/dimensions, optimize for BI queries

**Storage**: Data warehouse (Redshift in this example)

**Interface**: `RedshiftSparkInterface` (bulk load from Spark)

**Example DataObjects**:
- `users_agg` ‚Üí user metrics (total purchases, account age, etc.)
- `sales_daily_agg` ‚Üí daily sales rollup by region

```
Redshift: analytics_schema.
‚îú‚îÄ‚îÄ users_agg
‚îÇ   Columns: user_id, total_purchases, avg_order_value, ...
‚îú‚îÄ‚îÄ sales_daily_agg
‚îÇ   Columns: date, region, revenue, orders, ...
‚îî‚îÄ‚îÄ (other analytics tables)
```

---

## üîß How This Maps to Configuration

Each layer is declared once in a config file. DAO reads this and wires up the right interface for each store.

### The Config File (JSON)

```json
{
  "bronze": {
    "interface_class": "S3Boto3Interface",
    "interface_class_location": "interface_classes.s3.s3_boto3",
    "default_configs": { 
      "bucket": "my-company-bronze" 
    }
  },
  "silver": {
    "interface_class": "S3SparkInterface",
    "interface_class_location": "interface_classes.s3.spark",
    "default_configs": { 
      "bucket": "my-company-silver", 
      "prefix": "datasets" 
    }
  },
  "gold": {
    "interface_class": "RedshiftSparkInterface",
    "interface_class_location": "interface_classes.redshift.spark",
    "default_configs": { 
      "host": "redshift.company.redshift.amazonaws.com",
      "user": "pipeline_user",
      "password": "***",
      "database": "analytics",
      "s3_temp_dir": "s3://my-company-temp/redshift/",
      "iam_role_arn": "arn:aws:iam::123456789:role/redshift-role"
    }
  }
}
```

### How Config Becomes DAO

```
Config (JSON)
    ‚îÇ
    ‚îú‚îÄ "bronze" ‚îÄ‚îÄ> DataStore
    ‚îÇ              ‚îú‚îÄ Name: "bronze"
    ‚îÇ              ‚îú‚îÄ Interface: S3Boto3Interface
    ‚îÇ              ‚îî‚îÄ Defaults: { bucket: "my-company-bronze" }
    ‚îÇ
    ‚îú‚îÄ "silver" ‚îÄ‚îÄ> DataStore
    ‚îÇ              ‚îú‚îÄ Name: "silver"
    ‚îÇ              ‚îú‚îÄ Interface: S3SparkInterface
    ‚îÇ              ‚îî‚îÄ Defaults: { bucket: "my-company-silver", prefix: "datasets" }
    ‚îÇ
    ‚îî‚îÄ "gold" ‚îÄ‚îÄ‚îÄ> DataStore
                   ‚îú‚îÄ Name: "gold"
                   ‚îú‚îÄ Interface: RedshiftSparkInterface
                   ‚îî‚îÄ Defaults: { host: "...", user: "...", ... }

Later, when you create DataObjects:

DataObject("users_raw", bronze_store)
    ‚îú‚îÄ Name: "users_raw"
    ‚îú‚îÄ DataStore: bronze (knows S3Boto3Interface)
    ‚îî‚îÄ Ready to use: dao.read(users_raw, ...)

DataObject("users_clean", silver_store)
    ‚îú‚îÄ Name: "users_clean"
    ‚îú‚îÄ DataStore: silver (knows S3SparkInterface)
    ‚îî‚îÄ Ready to use: dao.read(users_clean, format='parquet')

DataObject("users_agg", gold_store)
    ‚îú‚îÄ Name: "users_agg"
    ‚îú‚îÄ DataStore: gold (knows RedshiftSparkInterface)
    ‚îî‚îÄ Ready to use: dao.write(df, users_agg)
```

### What Each Config Field Means

| Field | Purpose | Example |
|-------|---------|---------|
| `interface_class` | The class name that will handle I/O | `S3Boto3Interface` |
| `interface_class_location` | Module path to find the class | `interface_classes.s3.s3_boto3` |
| `default_configs` | Arguments passed to the interface constructor | `{ "bucket": "..." }` |

---

## üíª Writing the Pipeline Code

Once config is in place, your code becomes simple:

### Step 1: Initialize DAO with Your Config

```python
from dao.core.dao import DataAccessObject
from dao.data_object import DataObject
from dao.data_store import DataStoreRegistry

# Load config (JSON file or dict)
config = {
    "bronze": { ... },  # from above
    "silver": { ... },
    "gold": { ... }
}

# One line: DAO is ready
dao = DataAccessObject(config)
```

### Step 2: Ingest Raw Data to Bronze

```python
# Get the bronze store and create a data object
bronze_store = DataStoreRegistry.get('bronze')
users_raw = DataObject('users_raw', bronze_store)

# Upload a CSV file to S3 (uses S3Boto3Interface under the hood)
with open('sample/users.csv', 'rb') as f:
    dao.write(
        data_object=users_raw, 
        data=f, 
        path='s3://my-company-bronze/users_raw/2025-01-07/'
    )
# ‚úÖ File is now in S3 bronze layer
```

### Step 3: Transform to Silver (With Spark)

```python
# Read from bronze using Spark (S3SparkInterface)
users_df = dao.read(
    data_object=users_raw,
    format='csv'
    # S3SparkInterface knows the bucket/path from defaults
)

# Clean it up
users_clean = (users_df
    .dropDuplicates(['id'])  # remove duplicates
    .select('id', 'name', 'email', 'created_at')  # trim columns
    .filter("email IS NOT NULL")  # remove nulls
)

# Write to silver (parquet, partitioned by date)
silver_store = DataStoreRegistry.get('silver')
users_clean_obj = DataObject('users_clean', silver_store)

dao.write(
    data=users_clean,
    data_object=users_clean_obj,
    format='parquet',
    path='s3://my-company-silver/datasets/users_clean/',
    spark_options={'mode': 'overwrite'}
)
# ‚úÖ Cleaned data is now in S3 silver layer, ready for analytics
```

### Step 4: Load to Gold (Analytics-Ready in Redshift)

```python
# Read from silver
users_clean_data = dao.read(
    data_object=users_clean_obj,
    format='parquet'
)

# Aggregate: compute user metrics
from pyspark.sql.functions import col, count, avg, max

users_agg = (users_clean_data
    .groupBy('id')
    .agg(
        count('*').alias('total_purchases'),
        avg('order_value').alias('avg_order_value'),
        max('created_at').alias('last_purchase_date')
    )
)

# Write to Redshift (gold layer)
gold_store = DataStoreRegistry.get('gold')
users_agg_obj = DataObject('users_agg', gold_store)

dao.write(
    data=users_agg,
    data_object=users_agg_obj,
    format='parquet'  # or spark's native write
)
# ‚úÖ Aggregated metrics now live in Redshift for BI teams
```

### The Full Picture (How Routes Work)

```
Your Code          DAO Router              Interface           Storage
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
dao.read(users_raw,
  format='csv')  ‚îÄ‚îÄ‚îê
                   ‚îú‚îÄ‚îÄ> knows users_raw
                   ‚îÇ    is in bronze_store ‚îÄ‚îÄ> S3Boto3Interface ‚îÄ‚îÄ> S3
                   ‚îÇ
dao.write(users_clean,
  format='parquet') ‚îÄ‚îê
                     ‚îú‚îÄ‚îÄ> knows users_clean
                     ‚îÇ    is in silver_store ‚îÄ‚îÄ> S3SparkInterface ‚îÄ‚îÄ> S3
                     ‚îÇ
dao.write(users_agg) ‚îÄ‚îÄ‚îê
                        ‚îú‚îÄ‚îÄ> knows users_agg
                        ‚îÇ    is in gold_store ‚îÄ‚îÄ> RedshiftSparkInterface ‚îÄ‚îÄ> Redshift
```

---

## üéØ Key Design Patterns

### 1. Object Naming Across Layers
Keep the core entity name consistent, add suffixes for clarity:
- Bronze: `users_raw` (raw CSV/JSON)
- Silver: `users_clean` (cleaned parquet)
- Gold: `users_agg` (aggregated)

### 2. Default Configs Keep Code Clean
Define bucket names, prefixes, and connection strings once in config, not scattered in code:

```python
# Instead of repeating bucket names everywhere:
# ‚ùå Bad: dao.write(..., path='s3://my-company-silver/datasets/users_clean/')
# ‚úÖ Good: DAO uses config defaults, you just specify data_object and format
```

### 3. Let DAO Route
Pass `format` and `spark_options` only when you need format-specific control. Otherwise, let DAO pick the right interface:

```python
# ‚úÖ Simple: DAO auto-picks interface based on data_object's store
dao.read(data_object=my_data)

# ‚úÖ Specific: You tell DAO the format when needed
dao.read(data_object=my_data, format='parquet')
```

---

## üìñ Next Steps

Ready to see this in action? Check out the example scripts in `examples/medallion/`:
1. `01_ingest_bronze.py` ‚Äî upload raw data to bronze layer
2. `02_transform_silver.py` ‚Äî clean and partition to silver layer
3. `03_load_gold.py` ‚Äî aggregate and load to gold layer (Redshift)

See `catalog.json` for store and object configuration, and `sample_data/` for tiny test datasets.

To run locally:
- **Option A**: Use real AWS S3 (set `AWS_*` env vars)
- **Option B**: Use MinIO + docker-compose (portable, no AWS account needed)

Start with any example script and follow the inline comments.
